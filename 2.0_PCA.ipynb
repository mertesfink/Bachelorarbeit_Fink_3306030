{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06f081d-e7fd-44e3-8652-5fbf76c7ff7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PCA Regression FELT_LIFE_NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3274b5f-4682-44d6-adc4-142a2dbcd86d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import src.database\n",
    "from src.Dataset import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import src.plot \n",
    "from src import featureselection\n",
    "import shap\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94a3ce-c762-4fa9-b555-4ce029357825",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272e68b3-658b-453e-a4ab-d71e90869e06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Model Linear Regressor, Components: 10\n",
      "Finished: Model SVR, Components: 10\n",
      "Finished: Model Random Forest, Components: 10\n",
      "Finished: Model Gradient Boosting, Components: 10\n",
      "Finished: Model Linear Regressor, Components: 20\n",
      "Finished: Model SVR, Components: 20\n",
      "Finished: Model Random Forest, Components: 20\n",
      "Finished: Model Gradient Boosting, Components: 20\n",
      "Finished: Model Linear Regressor, Components: 30\n",
      "Finished: Model SVR, Components: 30\n",
      "Finished: Model Random Forest, Components: 30\n",
      "Finished: Model Gradient Boosting, Components: 30\n",
      "Finished: Model Linear Regressor, Components: 40\n",
      "Finished: Model SVR, Components: 40\n",
      "Finished: Model Random Forest, Components: 40\n",
      "Finished: Model Gradient Boosting, Components: 40\n",
      "Saved metrics and predictions for 10 components.\n",
      "Saved metrics and predictions for 20 components.\n",
      "Saved metrics and predictions for 30 components.\n",
      "Saved metrics and predictions for 40 components.\n"
     ]
    }
   ],
   "source": [
    "#Vorverarbeitete Daten aus CSV auslesen\n",
    "df = pd.read_csv(\"data/df_preprocessed_all.csv\", sep = \",\")\n",
    "df = df.drop(columns=['FELT_LIFE','REMOVAL_DATE', 'INSTALLATION_DATE', 'REPORT_DATE'])\n",
    "\n",
    "#Das Label nennen\n",
    "label = df.columns.get_loc('FELT_LIFE_NET')\n",
    "header = df.columns\n",
    "\n",
    "#Machine Learning Algorithms die benutzt werden\n",
    "MLA = [\n",
    "    ('Linear Regressor', LinearRegression()),\n",
    "    ('SVR', SVR()),\n",
    "    ('Random Forest', RandomForestRegressor()),\n",
    "    ('Gradient Boosting', xgb.XGBRegressor())\n",
    "]\n",
    "\n",
    "all_results = {}  # Speichert Ergebnisse für jede Hauptkomponentenanzahl\n",
    "all_predictions = {}  # Speichert Predictions für jede Hauptkomponentenanzahl\n",
    "\n",
    "# Feature-Anzahlen, die getestet werden sollen\n",
    "feature_counts = [\n",
    "    #1,2,3,4,5,6,7,8,9,\n",
    "    10, \n",
    "    20, \n",
    "    30, \n",
    "    40\n",
    "]\n",
    "\n",
    "for num_features in feature_counts:\n",
    "    \n",
    "    # Initialisiere Ergebnisse und Predictions für die aktuelle Hauptkomponentenanzahl\n",
    "    all_results[num_features] = {}\n",
    "    all_predictions[num_features] = {}\n",
    "    \n",
    "    # Für alle Modelle in MLA Trainvalidierung, Cross-Validierung und Testvalidierung \n",
    "    for model_name, model_instance in MLA:\n",
    "        \n",
    "        #Dataset vorbereiten\n",
    "        dataset = Dataset(df, 'df', label, divide_dataset=False, header=header)\n",
    "        dataset.divide_dataset(model_instance, normalize=False, shuffle=True, all_features=True, all_instances=True, evaluate=False, partial_sample=False,folds=5)\n",
    "    \n",
    "        #Dimension Reduction mit PCA\n",
    "        x_train_pca,  x_test_pca, duration, pca = featureselection.pca(dataset.get_X_train(),  dataset.get_X_test(), num_features)\n",
    "        \n",
    "        #print(pca.components_)\n",
    "        #print(pca.explained_variance_)\n",
    "        #print(sum(pca.explained_variance_ratio_))\n",
    "        \n",
    "        \n",
    "        dataset.set_X_train(x_train_pca)\n",
    "        dataset.set_X_test(x_test_pca)\n",
    "        \n",
    "        dataset.set_features(list(range(num_features)))\n",
    "       \n",
    "\n",
    "        #Regressor trainieren\n",
    "        dataset.fit_classifier()\n",
    "\n",
    "        # Cross-Validation\n",
    "        dataset.set_CV()\n",
    "        cv = dataset.get_CV()\n",
    "\n",
    "        #Validierung auf Testset\n",
    "        dataset.set_train_metrics()\n",
    "        train = dataset.get_train_metrics()\n",
    "        traintime = dataset.get_traintime()\n",
    "        \n",
    "\n",
    "        dataset.set_test_metrics()\n",
    "        test = dataset.get_test_metrics()\n",
    "\n",
    "        # Ergebnisse für das aktuelle Modell und die aktuelle Hauptkomponentenanzahl speichern\n",
    "        all_results[num_features][model_name] = {\n",
    "            'Model_name': model_name,\n",
    "            'CV_TrainMAE': cv['CV_TrainMAE'],\n",
    "            'CV_TrainRMSE': cv['CV_TrainRMSE'],\n",
    "            'CV_TestMAE': cv['CV_TestMAE'],\n",
    "            'CV_TestRMSE': cv['CV_TestRMSE'],\n",
    "            'CV_fit_time': cv['CV_fit_time'],\n",
    "            'CV_fit_time_ges':cv['CV_fit_time']+duration,\n",
    "            'TrainRMSE': train['TrainRMSE'],\n",
    "            'TrainMAE': train['TrainMAE'],\n",
    "            'TestRMSE': test['TestRMSE'],\n",
    "            'TestMAE': test['TestMAE'],\n",
    "            'TrainTime_ges': traintime,\n",
    "            'Hauptkomponenten': pca.explained_variance_ratio_,\n",
    "            'Hauptkomponenten-Anzahl': num_features,\n",
    "            'FS-Laufzeit': duration\n",
    "        }\n",
    "\n",
    "        # Predictions für das aktuelle Modell und die aktuelle Hauptkomponentenanzahl speichern\n",
    "        all_predictions[num_features][model_name] = {\n",
    "            'Model_name': model_name,\n",
    "            'y_train': dataset.get_y_train(),\n",
    "            'y_test': dataset.get_y_test(),\n",
    "            'pred_train': dataset.get_y_pred_train(),\n",
    "            'pred_test': dataset.get_y_pred_test()\n",
    "        }\n",
    "\n",
    "        print(f\"Finished: Model {model_name}, Components: {num_features}\")\n",
    "\n",
    "        \n",
    "# Ergebnisse und Predictions als CSV speichern\n",
    "\n",
    "output_dir = \"data/PCA/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ergebnisse für alle Hauptkomponentenanzahlen und Modelle\n",
    "for num_features, models_results in all_results.items():\n",
    "    # 1. Speichern der Metriken im gewünschten Format\n",
    "    df_metrics = pd.DataFrame(models_results)\n",
    "    df_metrics = df_metrics.transpose()  # Um sicherzustellen, dass jedes Modell als Zeile erscheint\n",
    "    df_metrics.to_csv(f'{output_dir}{num_features}_PCA_metrics.csv', index=False)\n",
    "\n",
    "    # 2. Speichern der Vorhersagen im flachen Format (DataFrame für Vorhersagen)\n",
    "    flattened_predictions = []\n",
    "    \n",
    "    for model_name, data in all_predictions[num_features].items():\n",
    "        for data_type, values in data.items():\n",
    "            if data_type != 'Model_name':  # Überspringe das Model_name-Feld\n",
    "                for value in values:\n",
    "                    flattened_predictions.append({\n",
    "                        'Model_name': model_name,\n",
    "                        'Data_type': data_type,\n",
    "                        'Value': value\n",
    "                    })\n",
    "    \n",
    "    df_predictions = pd.DataFrame(flattened_predictions)\n",
    "    df_predictions.to_csv(f'{output_dir}{num_features}_PCA_predictions.csv', index=False)\n",
    "\n",
    "    print(f\"Saved metrics and predictions for {num_features} components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a708709-8fa9-455f-8ae9-06da6d4cb2f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ShapleyValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd523d-3906-42ed-ab65-37a82a0e4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vorverarbeitete Daten aus CSV auslesen\n",
    "df = pd.read_csv(\"data/df_preprocessed_all.csv\", sep = \",\")\n",
    "df = df.drop(columns=['FELT_LIFE','REMOVAL_DATE', 'INSTALLATION_DATE', 'REPORT_DATE'])\n",
    "\n",
    "#Das Label nennen\n",
    "label = df.columns.get_loc('FELT_LIFE_NET')\n",
    "header = df.columns\n",
    "\n",
    "#Machine Learning Algorithms die benutzt werden\n",
    "MLA = [\n",
    "    #('Linear Regressor', LinearRegression()),\n",
    "    #('SVR', SVR()),\n",
    "    #('Random Forest', RandomForestRegressor()),\n",
    "    ('Gradient Boosting', xgb.XGBRegressor())\n",
    "]\n",
    "\n",
    "# Feature-Anzahlen, die getestet werden sollen\n",
    "feature_counts = [\n",
    "    5,\n",
    "    #10, \n",
    "    #20, \n",
    "    #30, \n",
    "    #40\n",
    "]\n",
    "\n",
    "for num_features in feature_counts:\n",
    "    \n",
    "    # Für alle Modelle in MLA Trainvalidierung, Cross-Validierung und Testvalidierung \n",
    "    for model_name, model_instance in MLA:\n",
    "        \n",
    "        #Dataset vorbereiten\n",
    "        dataset = Dataset(df, 'df', label, divide_dataset=False, header=header)\n",
    "        dataset.divide_dataset(model_instance, normalize=False, shuffle=True, all_features=True, all_instances=True, evaluate=False, partial_sample=False,folds=5)\n",
    "        \n",
    "        #Dimension Reduction mit PCA\n",
    "        x_train_org = dataset.get_X_train()\n",
    "        x_train_pca, x_test_pca, duration, pca = featureselection.pca(dataset.get_X_train(), dataset.get_X_val(), dataset.get_X_test(), num_features)\n",
    "        \n",
    "        dataset.set_X_train(x_train_pca)\n",
    "        dataset.set_features(list(range(num_features)))\n",
    "       \n",
    "        #Regressor trainieren\n",
    "        dataset.fit_classifier()\n",
    "        \n",
    "        #Berechnung und Rücktransformation auf die ursprünglichen Features\n",
    "        shap_values_pca = dataset.shapley_values()\n",
    "        shap_values_original = shap_values_pca.dot(pca.components_)\n",
    "        \n",
    "\n",
    "        # Visualisierung \n",
    "        shap.summary_plot(shap_values_original, x_train_org, feature_names=df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
